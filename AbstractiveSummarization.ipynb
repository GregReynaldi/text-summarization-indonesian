{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623d8add",
   "metadata": {},
   "source": [
    "## Abstractive Summarization : T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc15b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\ReynaldiENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer,Seq2SeqTrainingArguments, BartTokenizer, BartForConditionalGeneration, DataCollatorForSeq2Seq, pipeline\n",
    "from datasets import Dataset, DatasetDict\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from rouge_score import rouge_scorer\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fcc459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_dev = os.listdir(os.path.join(os.path.abspath(os.getcwd()),\"./liputan6_data/canonical/dev\"))\n",
    "canonical_test = os.listdir(os.path.join(os.path.abspath(os.getcwd()),\"./liputan6_data/canonical/test\"))\n",
    "canonical_train = os.listdir(os.path.join(os.path.abspath(os.getcwd()),\"./liputan6_data/canonical/train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "badf1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText(text) : \n",
    "    text = re.sub(pattern = r\"\\(\", repl = r\"\", string = text)\n",
    "    text = re.sub(pattern = r\"\\)\", repl = r\"\", string = text)\n",
    "    text = re.sub(pattern = r\"\\s\\.\", repl = \".\", string = text)\n",
    "    text = re.sub(pattern = r\" , \", repl = \", \", string = text)\n",
    "    text = re.sub(pattern = r\"\\s\\?\", repl = \"?\", string = text)\n",
    "    text = re.sub(pattern = r\"\\s!\", repl = \"!\", string = text)\n",
    "    text = re.sub(pattern = r\"\\s+\", repl = \" \", string = text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a016e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_article_list = []\n",
    "clean_summary = []\n",
    "kunci_jawaban = []\n",
    "clean_article = []\n",
    "\n",
    "for fileJSON in canonical_train : \n",
    "    with open(os.path.join(os.path.abspath(os.getcwd()),\"./liputan6_data/canonical/train\",fileJSON),\"r\") as f : \n",
    "        file = json.load(f)\n",
    "        clean_article_list.append([])\n",
    "        sentence_result = \"\" \n",
    "        for sentence in file['clean_article'] : \n",
    "            sentence_result = \" \".join(sentence)\n",
    "            sentence_result = preprocessingText(sentence_result)\n",
    "            clean_article_list[-1].append(sentence_result.lower())\n",
    "            \n",
    "        sentence_result = \"\" \n",
    "        for sentence in file['clean_summary'] : \n",
    "            sentence_result+=\" \".join(sentence)\n",
    "            sentence_result+=\" \"\n",
    "        sentence_result = preprocessingText(sentence_result)\n",
    "        clean_summary.append(sentence_result.lower())\n",
    "        kunci_jawaban.append(file[\"extractive_summary\"])\n",
    "for article in clean_article_list : \n",
    "    clean_article.append(\" \".join(article))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c149b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_article_list_test = []\n",
    "clean_summary_test = []\n",
    "kunci_jawaban_test = []\n",
    "clean_article_test = []\n",
    "\n",
    "for fileJSON in canonical_test : \n",
    "    with open(os.path.join(os.path.abspath(os.getcwd()),\"./liputan6_data/canonical/test\",fileJSON),\"r\") as f : \n",
    "        file = json.load(f)\n",
    "        clean_article_list_test.append([])\n",
    "        sentence_result = \"\" \n",
    "        for sentence in file['clean_article'] : \n",
    "            sentence_result = \" \".join(sentence)\n",
    "            sentence_result = preprocessingText(sentence_result)\n",
    "            clean_article_list_test[-1].append(sentence_result.lower())\n",
    "            \n",
    "        sentence_result = \"\" \n",
    "        for sentence in file['clean_summary'] : \n",
    "            sentence_result+=\" \".join(sentence)\n",
    "            sentence_result+=\" \"\n",
    "        sentence_result = preprocessingText(sentence_result)\n",
    "        clean_summary_test.append(sentence_result.lower())\n",
    "        kunci_jawaban_test.append(file[\"extractive_summary\"])\n",
    "for article in clean_article_list_test : \n",
    "    clean_article_test.append(\" \".join(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe169992",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"panggi/t5-small-indonesian-summarization-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e7b513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a307075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_dict({\"text\":clean_article, \"summary\":clean_summary})\n",
    "dataset_test = Dataset.from_dict({\"text\":clean_article_test, \"summary\":clean_summary_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9fa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_train = list(range(0,dataset_train.shape[0]))\n",
    "dataset_train = dataset_train.select(random.sample(range_train, k = 1000))\n",
    "\n",
    "range_test = list(range(0,dataset_test.shape[0]))\n",
    "dataset_test = dataset_test.select(random.sample(range_test, k = 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f7bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full = DatasetDict({\"train\":dataset_train,\"test\":dataset_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce4915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizerFn(batch) : \n",
    "    inputs = model_tokenizer(batch[\"text\"],padding = \"max_length\", truncation = True, max_length = 384)\n",
    "    summary = model_tokenizer(batch[\"summary\"], padding = \"max_length\", truncation = True, max_length = 160)\n",
    "    inputs[\"labels\"] = summary[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eadc498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:14<00:00, 565.42 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:02<00:00, 674.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = dataset_full.map(tokenizerFn, batched = True, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ee8a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCollator = DataCollatorForSeq2Seq(model = model_base, tokenizer = model_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ac958ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingArgs = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./abstractiveSummary-trainer\",\n",
    "    do_train = True, \n",
    "    do_eval = True,\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    fp16 = True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    metric_for_best_model = \"rouge_calculation\",\n",
    "    greater_is_better=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=160\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5401eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fn(eval_pred) : \n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.where(predictions == -100, model_tokenizer.pad_token_id, predictions)\n",
    "    prediction = model_tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
    "    label = np.where(labels != -100, labels, model_tokenizer.pad_token_id)\n",
    "    label = model_tokenizer.batch_decode(label, skip_special_tokens = True)\n",
    "    rougeScorer = rouge_scorer.RougeScorer(rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"])\n",
    "    hasil_list = []\n",
    "    for p,l in zip(prediction, label) : \n",
    "        rouge1 = rougeScorer.score(p, l)[\"rouge1\"].fmeasure\n",
    "        rouge2 = rougeScorer.score(p, l)[\"rouge2\"].fmeasure\n",
    "        rougeL = rougeScorer.score(p, l)[\"rougeL\"].fmeasure\n",
    "        hasil_list.append(np.mean([rouge1,rouge2,rougeL]))\n",
    "    return {\"rouge_calculation\":np.mean(hasil_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d630a04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reynaldi\\AppData\\Local\\Temp\\ipykernel_16116\\3047896467.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args = trainingArgs,\n",
    "    model = model_base,\n",
    "    tokenizer = model_tokenizer,\n",
    "    train_dataset = dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    "    compute_metrics = compute_fn,\n",
    "    data_collator = dataCollator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"abstractiveModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3898e",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "face539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_test = os.listdir(os.path.join(os.path.abspath(os.getcwd()),\"./liputan6_data/canonical/test\"))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa32163",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougeScorer = rouge_scorer.RougeScorer(rouge_types=[\"rougeL\",\"rouge1\",\"rouge2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b03851b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenizer_saved = T5Tokenizer.from_pretrained(\"abstractiveModel\")\n",
    "model_base_saved = T5ForConditionalGeneration.from_pretrained(\"abstractiveModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82028dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_summarization(batch) : \n",
    "    inputs =  model_tokenizer_saved(batch[\"text\"], padding = \"max_length\", truncation = True, max_length = 384, return_tensors = \"pt\")\n",
    "    with torch.no_grad() : \n",
    "        summarized_text = model_base_saved.generate(**inputs, max_length = 160)\n",
    "    summarized_text = model_tokenizer_saved.batch_decode(summarized_text, skip_special_tokens=True)\n",
    "    return {\"summary_predicted\":summarized_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8dc4226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = dataset_full[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e8e6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,  290,   15,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   6%|▋         | 128/2000 [00:19<04:46,  6.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  13%|█▎        | 256/2000 [00:38<04:23,  6.61 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  19%|█▉        | 384/2000 [00:57<04:02,  6.67 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,   18,  170,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  26%|██▌       | 512/2000 [01:16<03:41,  6.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5694,   323,     3,  ..., 10912,  6323,     1],\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ...,     1,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  32%|███▏      | 640/2000 [01:35<03:22,  6.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  38%|███▊      | 768/2000 [01:54<03:03,  6.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  45%|████▍     | 896/2000 [02:13<02:44,  6.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,   18,   19,    1],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,   14, 2363,    1],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  51%|█████     | 1024/2000 [02:33<02:26,  6.67 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  58%|█████▊    | 1152/2000 [02:51<02:06,  6.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  64%|██████▍   | 1280/2000 [03:10<01:45,  6.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ..., 4274, 9133,    1],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    7,  695,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  70%|███████   | 1408/2000 [03:29<01:27,  6.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ..., 7902,   13,    1],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  77%|███████▋  | 1536/2000 [03:47<01:07,  6.84 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  83%|████████▎ | 1664/2000 [04:06<00:49,  6.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ..., 3239,   17,    1],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  90%|████████▉ | 1792/2000 [04:25<00:30,  6.82 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0],\n",
      "        [5694,  323,    3,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  96%|█████████▌| 1920/2000 [04:44<00:11,  6.75 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ..., 16780,   330,     1],\n",
      "        ...,\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0],\n",
      "        [ 5694,   323,     3,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [04:58<00:00,  6.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "datasetCopy = datasetCopy.map(do_summarization, batched = True, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "062f2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_list = []\n",
    "r2_list = []\n",
    "rl_list = []\n",
    "for p,l in zip(datasetCopy[\"summary_predicted\"][:], datasetCopy[\"summary\"][:]) : \n",
    "    r1_list.append(rougeScorer.score(p,l)[\"rouge1\"].fmeasure)\n",
    "    r2_list.append(rougeScorer.score(p,l)[\"rouge2\"].fmeasure)\n",
    "    rl_list.append(rougeScorer.score(p,l)[\"rougeL\"].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e4143fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil ROUGE: \n",
      "Rouge1 : 0.3792692577633349\n",
      "Rouge2 : 0.20946316813588345\n",
      "RougeL : 0.31032809118845567\n"
     ]
    }
   ],
   "source": [
    "print(\"Hasil ROUGE: \")\n",
    "print(f\"Rouge1 : {mean(r1_list)}\")\n",
    "print(f\"Rouge2 : {mean(r2_list)}\")\n",
    "print(f\"RougeL : {mean(rl_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c4664",
   "metadata": {},
   "source": [
    "## Abstractive Summarization : Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820ccb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\ReynaldiENV\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85136392",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_id = \"cahya/bert2gpt-indonesian-summarization\"\n",
    "decoder_id = \"cahya/bert2gpt-indonesian-summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8293f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\ReynaldiENV\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Reynaldi\\.cache\\huggingface\\hub\\models--cahya--bert2gpt-indonesian-summarization. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The encoder model config class: <class 'transformers.models.bert.configuration_bert.BertConfig'> is different from the decoder model config class: <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>. It is not recommended to use the `AutoTokenizer.from_pretrained()` method in this case. Please use the encoder and decoder specific tokenizer classes.\n"
     ]
    }
   ],
   "source": [
    "model_tokenizer = AutoTokenizer.from_pretrained(encoder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ae29ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = EncoderDecoderModel.from_pretrained(encoder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f66a6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base.config.decoder_start_token_id = model_tokenizer.cls_token_id\n",
    "model_base.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "model_base.config.eos_token_id = model_tokenizer.sep_token_id\n",
    "model_base.vocab_size = model_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a10bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_summarization(batch) : \n",
    "    inputs =  model_tokenizer(batch[\"text\"], padding = \"max_length\", truncation = True, max_length = 384, return_tensors = \"pt\")\n",
    "    with torch.no_grad() : \n",
    "        summarized_text = model_base.generate(**inputs, max_length = 160)\n",
    "    summarized_text = model_tokenizer.batch_decode(summarized_text, skip_special_tokens=True)\n",
    "    return {\"summary_predicted\":summarized_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4da7a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = dataset_full[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5700cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCopy = datasetCopy.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28889c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [02:51<00:00,  1.72s/ examples]\n"
     ]
    }
   ],
   "source": [
    "datasetCopy = datasetCopy.map(do_summarization, batched = True, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfd5916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougeScorer = rouge_scorer.RougeScorer(rouge_types=[\"rougeL\",\"rouge1\",\"rouge2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2886d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_list = []\n",
    "r2_list = []\n",
    "rl_list = []\n",
    "for p,l in zip(datasetCopy[\"summary_predicted\"][:], datasetCopy[\"summary\"][:]) : \n",
    "    r1_list.append(rougeScorer.score(p,l)[\"rouge1\"].fmeasure)\n",
    "    r2_list.append(rougeScorer.score(p,l)[\"rouge2\"].fmeasure)\n",
    "    rl_list.append(rougeScorer.score(p,l)[\"rougeL\"].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f1ea51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil ROUGE: \n",
      "Rouge1 : 0.4259107944700765\n",
      "Rouge2 : 0.2486188432290399\n",
      "RougeL : 0.35874525263007095\n"
     ]
    }
   ],
   "source": [
    "print(\"Hasil ROUGE: \")\n",
    "print(f\"Rouge1 : {mean(r1_list)}\")\n",
    "print(f\"Rouge2 : {mean(r2_list)}\")\n",
    "print(f\"RougeL : {mean(rl_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReynaldiENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
